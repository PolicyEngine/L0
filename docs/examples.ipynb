{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L0 Regularization Examples\n",
    "\n",
    "This notebook demonstrates L0 regularization with practical examples, including reproductions of key results from {cite}`louizos2018learning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import L0 modules\n",
    "from l0.layers import L0Linear, L0Conv2d, SparseMLP\n",
    "from l0.gates import FeatureGate, SampleGate\n",
    "from l0.penalties import (\n",
    "    compute_l0_penalty,\n",
    "    get_sparsity_stats,\n",
    "    TemperatureScheduler,\n",
    "    update_temperatures\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic L0 Linear Layer\n",
    "\n",
    "Let's start with a simple example showing how L0 regularization learns sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "n_samples = 1000\n",
    "n_features = 100\n",
    "n_informative = 10  # Only 10 features are actually useful\n",
    "\n",
    "# Generate data where only first 10 features matter\n",
    "X = torch.randn(n_samples, n_features)\n",
    "true_weights = torch.zeros(n_features)\n",
    "true_weights[:n_informative] = torch.randn(n_informative) * 2\n",
    "y = X @ true_weights + torch.randn(n_samples) * 0.1\n",
    "\n",
    "# Create train/test split\n",
    "train_X, test_X = X[:800], X[800:]\n",
    "train_y, test_y = y[:800], y[800:]\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"True informative features: {n_informative}/{n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create L0 regularized linear model\n",
    "class L0LinearRegression(nn.Module):\n",
    "    def __init__(self, n_features, init_sparsity=0.9):\n",
    "        super().__init__()\n",
    "        self.l0_linear = L0Linear(\n",
    "            n_features, 1,\n",
    "            bias=False,\n",
    "            init_sparsity=init_sparsity,\n",
    "            temperature=0.5\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l0_linear(x).squeeze()\n",
    "    \n",
    "    def get_l0_loss(self):\n",
    "        return self.l0_linear.get_l0_penalty()\n",
    "    \n",
    "    def get_sparsity(self):\n",
    "        return self.l0_linear.get_sparsity()\n",
    "\n",
    "model = L0LinearRegression(n_features, init_sparsity=0.5).to(device)\n",
    "print(f\"Initial sparsity: {model.get_sparsity():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with L0 regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "l0_lambda = 0.01  # L0 regularization strength\n",
    "\n",
    "losses = []\n",
    "sparsities = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    pred = model(train_X.to(device))\n",
    "    mse_loss = F.mse_loss(pred, train_y.to(device))\n",
    "    l0_loss = model.get_l0_loss()\n",
    "    total_loss = mse_loss + l0_lambda * l0_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(total_loss.item())\n",
    "    sparsities.append(model.get_sparsity())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, Sparsity={sparsities[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(losses)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Total Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(sparsities)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Sparsity')\n",
    "ax2.set_title('Learned Sparsity')\n",
    "ax2.grid(True)\n",
    "ax2.axhline(y=(n_features-n_informative)/n_features, color='r', linestyle='--', \n",
    "            label=f'True sparsity: {(n_features-n_informative)/n_features:.1%}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final sparsity: {model.get_sparsity():.2%}\")\n",
    "print(f\"True sparsity: {(n_features-n_informative)/n_features:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which features were selected\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    gates = model.l0_linear.weight_gates()\n",
    "    active_features = (gates.squeeze() > 0.5).cpu().numpy()\n",
    "    \n",
    "print(f\"Active features: {active_features.sum()}/{n_features}\")\n",
    "print(f\"Correctly identified informative features: {active_features[:n_informative].sum()}/{n_informative}\")\n",
    "print(f\"False positives: {active_features[n_informative:].sum()}/{n_features-n_informative}\")\n",
    "\n",
    "# Visualize gate values\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.bar(range(n_features), gates.squeeze().cpu().numpy())\n",
    "plt.axvline(x=n_informative-0.5, color='r', linestyle='--', label='True/Noise boundary')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Gate Value')\n",
    "plt.title('Learned Gate Values (First 10 features are informative)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LeNet-5 on MNIST with L0\n",
    "\n",
    "Reproducing a simplified version of the LeNet experiments from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MNIST data (using random data for demo - replace with real MNIST)\n",
    "# In practice, use torchvision.datasets.MNIST\n",
    "mnist_train_X = torch.randn(1000, 1, 28, 28)\n",
    "mnist_train_y = torch.randint(0, 10, (1000,))\n",
    "mnist_test_X = torch.randn(200, 1, 28, 28)\n",
    "mnist_test_y = torch.randint(0, 10, (200,))\n",
    "\n",
    "train_dataset = TensorDataset(mnist_train_X, mnist_train_y)\n",
    "test_dataset = TensorDataset(mnist_test_X, mnist_test_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L0LeNet5(nn.Module):\n",
    "    \"\"\"LeNet-5 with L0 regularization on conv and fc layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, init_sparsity=0.5, temperature=0.5):\n",
    "        super().__init__()\n",
    "        # Conv layers with L0\n",
    "        self.conv1 = L0Conv2d(1, 6, 5, init_sparsity=init_sparsity, temperature=temperature)\n",
    "        self.conv2 = L0Conv2d(6, 16, 5, init_sparsity=init_sparsity, temperature=temperature)\n",
    "        \n",
    "        # FC layers with L0\n",
    "        self.fc1 = L0Linear(16 * 4 * 4, 120, init_sparsity=init_sparsity, temperature=temperature)\n",
    "        self.fc2 = L0Linear(120, 84, init_sparsity=init_sparsity, temperature=temperature)\n",
    "        self.fc3 = L0Linear(84, 10, init_sparsity=init_sparsity, temperature=temperature)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_l0_loss(self):\n",
    "        l0_loss = 0\n",
    "        for module in self.modules():\n",
    "            if hasattr(module, 'get_l0_penalty'):\n",
    "                l0_loss = l0_loss + module.get_l0_penalty()\n",
    "        return l0_loss\n",
    "    \n",
    "    def get_layer_sparsities(self):\n",
    "        sparsities = {}\n",
    "        for name, module in self.named_modules():\n",
    "            if hasattr(module, 'get_sparsity'):\n",
    "                sparsities[name] = module.get_sparsity()\n",
    "        return sparsities\n",
    "\n",
    "lenet = L0LeNet5(init_sparsity=0.0, temperature=0.5).to(device)\n",
    "print(\"Initial layer sparsities:\")\n",
    "for name, sparsity in lenet.get_layer_sparsities().items():\n",
    "    print(f\"  {name}: {sparsity:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training LeNet with L0 and temperature annealing\n",
    "optimizer = optim.Adam(lenet.parameters(), lr=0.001)\n",
    "temp_scheduler = TemperatureScheduler(initial_temp=1.0, final_temp=0.05, anneal_epochs=20)\n",
    "l0_lambda = 1e-3\n",
    "\n",
    "train_losses = []\n",
    "layer_sparsities = {name: [] for name in lenet.get_layer_sparsities().keys()}\n",
    "\n",
    "for epoch in range(30):\n",
    "    # Update temperature\n",
    "    temp = temp_scheduler.get_temperature(epoch)\n",
    "    update_temperatures(lenet, temp)\n",
    "    \n",
    "    # Training\n",
    "    lenet.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = lenet(batch_x)\n",
    "        ce_loss = F.cross_entropy(output, batch_y)\n",
    "        l0_loss = lenet.get_l0_loss()\n",
    "        total_loss = ce_loss + l0_lambda * l0_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += total_loss.item()\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    \n",
    "    # Track sparsities\n",
    "    for name, sparsity in lenet.get_layer_sparsities().items():\n",
    "        layer_sparsities[name].append(sparsity)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_sparsity = np.mean(list(lenet.get_layer_sparsities().values()))\n",
    "        print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f}, Temp={temp:.3f}, Avg Sparsity={avg_sparsity:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer-wise sparsity evolution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('LeNet-5 Training with L0')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, sparsity_history in layer_sparsities.items():\n",
    "    plt.plot(sparsity_history, label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Sparsity')\n",
    "plt.title('Layer-wise Sparsity Evolution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final sparsities\n",
    "print(\"\\nFinal layer sparsities:\")\n",
    "total_params = 0\n",
    "active_params = 0\n",
    "for name, module in lenet.named_modules():\n",
    "    if hasattr(module, 'get_sparsity'):\n",
    "        sparsity = module.get_sparsity()\n",
    "        if hasattr(module, 'weight'):\n",
    "            n_params = module.weight.numel()\n",
    "            n_active = int(n_params * (1 - sparsity))\n",
    "            total_params += n_params\n",
    "            active_params += n_active\n",
    "            print(f\"  {name}: {sparsity:.2%} sparse ({n_active}/{n_params} active)\")\n",
    "\n",
    "print(f\"\\nOverall sparsity: {1 - active_params/total_params:.2%}\")\n",
    "print(f\"Compression ratio: {total_params/active_params:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection with L0\n",
    "\n",
    "Using L0 gates for feature selection in tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create high-dimensional data with few informative features\n",
    "n_samples = 500\n",
    "n_features = 1000\n",
    "n_informative = 20\n",
    "\n",
    "# Generate classification data\n",
    "X = torch.randn(n_samples, n_features)\n",
    "true_weights = torch.zeros(n_features)\n",
    "true_weights[:n_informative] = torch.randn(n_informative) * 3\n",
    "logits = X @ true_weights\n",
    "y = (torch.sigmoid(logits) > 0.5).long()\n",
    "\n",
    "feature_names = [f\"feature_{i}\" for i in range(n_features)]\n",
    "print(f\"Dataset: {n_samples} samples, {n_features} features ({n_informative} informative)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection with FeatureGate\n",
    "feature_gate = FeatureGate(\n",
    "    n_features=n_features,\n",
    "    max_features=50,  # Select at most 50 features\n",
    "    temperature=0.2\n",
    ").to(device)\n",
    "\n",
    "# Simple classifier on top of selected features\n",
    "class FeatureSelectClassifier(nn.Module):\n",
    "    def __init__(self, feature_gate):\n",
    "        super().__init__()\n",
    "        self.feature_gate = feature_gate\n",
    "        self.classifier = nn.Linear(n_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply feature gates\n",
    "        gates = self.feature_gate.get_gates()\n",
    "        x_gated = x * gates\n",
    "        return self.classifier(x_gated).squeeze()\n",
    "\n",
    "model = FeatureSelectClassifier(feature_gate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(X.to(device))\n",
    "    bce_loss = F.binary_cross_entropy_with_logits(output, y.float().to(device))\n",
    "    l0_loss = feature_gate.get_penalty()\n",
    "    total_loss = bce_loss + 0.001 * l0_loss\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        n_selected = feature_gate.get_num_active()\n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, Selected features={n_selected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze selected features\n",
    "model.eval()\n",
    "importance = feature_gate.get_feature_importance()\n",
    "top_30_indices = torch.topk(importance, 30).indices.cpu().numpy()\n",
    "\n",
    "print(f\"\\nTop 30 selected features:\")\n",
    "print(f\"True informative features in top 30: {sum(i < n_informative for i in top_30_indices)}/{n_informative}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(100), importance[:100].cpu().numpy())\n",
    "plt.axvline(x=n_informative-0.5, color='r', linestyle='--', label='True/Noise boundary')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance (Gate Value)')\n",
    "plt.title('Feature Importance (First 100 features)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show importance distribution\n",
    "plt.hist(importance.cpu().numpy(), bins=50, edgecolor='black')\n",
    "plt.xlabel('Importance Value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Feature Importance')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Selection threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing L0 with L1 Regularization\n",
    "\n",
    "A direct comparison showing the difference between L0 and L1 sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for comparison\n",
    "class L1Linear(nn.Module):\n",
    "    \"\"\"Standard linear layer with L1 regularization.\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x).squeeze()\n",
    "    \n",
    "    def get_l1_penalty(self):\n",
    "        return self.linear.weight.abs().sum()\n",
    "    \n",
    "    def get_sparsity(self, threshold=1e-3):\n",
    "        return (self.linear.weight.abs() < threshold).float().mean().item()\n",
    "\n",
    "# Train both models on the same data\n",
    "n_features = 100\n",
    "X_train = torch.randn(500, n_features)\n",
    "y_train = torch.randn(500)\n",
    "\n",
    "# L0 model\n",
    "l0_model = L0LinearRegression(n_features, init_sparsity=0.0).to(device)\n",
    "l0_optimizer = optim.Adam(l0_model.parameters(), lr=0.01)\n",
    "\n",
    "# L1 model\n",
    "l1_model = L1Linear(n_features, 1).to(device)\n",
    "l1_optimizer = optim.Adam(l1_model.parameters(), lr=0.01)\n",
    "\n",
    "l0_sparsities = []\n",
    "l1_sparsities = []\n",
    "\n",
    "print(\"Training models...\")\n",
    "for epoch in range(100):\n",
    "    # Train L0\n",
    "    l0_optimizer.zero_grad()\n",
    "    l0_pred = l0_model(X_train.to(device))\n",
    "    l0_mse = F.mse_loss(l0_pred, y_train.to(device))\n",
    "    l0_total = l0_mse + 0.01 * l0_model.get_l0_loss()\n",
    "    l0_total.backward()\n",
    "    l0_optimizer.step()\n",
    "    l0_sparsities.append(l0_model.get_sparsity())\n",
    "    \n",
    "    # Train L1\n",
    "    l1_optimizer.zero_grad()\n",
    "    l1_pred = l1_model(X_train.to(device))\n",
    "    l1_mse = F.mse_loss(l1_pred, y_train.to(device))\n",
    "    l1_total = l1_mse + 0.01 * l1_model.get_l1_penalty()\n",
    "    l1_total.backward()\n",
    "    l1_optimizer.step()\n",
    "    l1_sparsities.append(l1_model.get_sparsity())\n",
    "\n",
    "print(f\"\\nFinal L0 sparsity: {l0_sparsities[-1]:.2%}\")\n",
    "print(f\"Final L1 sparsity: {l1_sparsities[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weight distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# L0 weights\n",
    "l0_model.eval()\n",
    "with torch.no_grad():\n",
    "    l0_weights = l0_model.l0_linear.weight.squeeze().cpu().numpy()\n",
    "    l0_gates = l0_model.l0_linear.weight_gates().squeeze().cpu().numpy()\n",
    "    l0_effective = l0_weights * l0_gates\n",
    "\n",
    "# L1 weights\n",
    "l1_weights = l1_model.linear.weight.squeeze().detach().cpu().numpy()\n",
    "\n",
    "# Plot weight distributions\n",
    "axes[0, 0].hist(l0_effective, bins=50, edgecolor='black')\n",
    "axes[0, 0].set_title('L0: Effective Weight Distribution')\n",
    "axes[0, 0].set_xlabel('Weight Value')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "axes[0, 1].hist(l1_weights, bins=50, edgecolor='black')\n",
    "axes[0, 1].set_title('L1: Weight Distribution')\n",
    "axes[0, 1].set_xlabel('Weight Value')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "# Plot sparsity evolution\n",
    "axes[1, 0].plot(l0_sparsities, label='L0', linewidth=2)\n",
    "axes[1, 0].plot(l1_sparsities, label='L1', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Sparsity')\n",
    "axes[1, 0].set_title('Sparsity Evolution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot gate distribution for L0\n",
    "axes[1, 1].hist(l0_gates, bins=50, edgecolor='black')\n",
    "axes[1, 1].set_title('L0: Gate Value Distribution')\n",
    "axes[1, 1].set_xlabel('Gate Value')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].axvline(x=0.5, color='r', linestyle='--', label='Threshold')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count exact zeros\n",
    "l0_zeros = (l0_gates < 0.5).sum()\n",
    "l1_zeros = (np.abs(l1_weights) < 1e-3).sum()\n",
    "print(f\"\\nExact zeros - L0: {l0_zeros}/{n_features}, L1: {l1_zeros}/{n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Structured Sparsity with L0\n",
    "\n",
    "Demonstrating channel-wise structured sparsity for CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN with structured sparsity\n",
    "class StructuredSparseCNN(nn.Module):\n",
    "    def __init__(self, structured=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = L0Conv2d(3, 32, 3, structured=structured, init_sparsity=0.3)\n",
    "        self.conv2 = L0Conv2d(32, 64, 3, structured=structured, init_sparsity=0.5)\n",
    "        self.conv3 = L0Conv2d(64, 128, 3, structured=structured, init_sparsity=0.7)\n",
    "        self.structured = structured\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return x\n",
    "    \n",
    "    def get_active_channels(self):\n",
    "        \"\"\"Count active channels in each layer.\"\"\"\n",
    "        active = {}\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, L0Conv2d):\n",
    "                if self.structured and hasattr(module, 'channel_gates'):\n",
    "                    gates = module.channel_gates.get_active_prob()\n",
    "                    active[name] = (gates > 0.5).sum().item()\n",
    "                else:\n",
    "                    # For unstructured, estimate from weight gates\n",
    "                    gates = module.weight_gates()\n",
    "                    # Approximate active channels\n",
    "                    channel_activity = gates.mean(dim=1)  # Average over input channels and kernel\n",
    "                    active[name] = (channel_activity > 0.5).sum().item()\n",
    "        return active\n",
    "\n",
    "# Compare structured vs unstructured\n",
    "structured_cnn = StructuredSparseCNN(structured=True).to(device)\n",
    "unstructured_cnn = StructuredSparseCNN(structured=False).to(device)\n",
    "\n",
    "print(\"Initial active channels:\")\n",
    "print(f\"Structured: {structured_cnn.get_active_channels()}\")\n",
    "print(f\"Unstructured: {unstructured_cnn.get_active_channels()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize structured vs unstructured sparsity patterns\n",
    "def visualize_conv_sparsity(model, layer_name='conv2'):\n",
    "    \"\"\"Visualize sparsity pattern in a conv layer.\"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name and isinstance(module, L0Conv2d):\n",
    "            module.eval()\n",
    "            with torch.no_grad():\n",
    "                if model.structured and hasattr(module, 'channel_gates'):\n",
    "                    # Structured: show channel gates\n",
    "                    gates = module.channel_gates().cpu().numpy()\n",
    "                    plt.figure(figsize=(10, 2))\n",
    "                    plt.bar(range(len(gates)), gates)\n",
    "                    plt.xlabel('Output Channel')\n",
    "                    plt.ylabel('Gate Value')\n",
    "                    plt.title(f'{layer_name} Channel Gates (Structured)')\n",
    "                    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "                else:\n",
    "                    # Unstructured: show weight gate heatmap\n",
    "                    gates = module.weight_gates()\n",
    "                    # Reshape to (out_channels, -1) for visualization\n",
    "                    gates_2d = gates.view(gates.shape[0], -1).cpu().numpy()\n",
    "                    \n",
    "                    plt.figure(figsize=(10, 4))\n",
    "                    plt.imshow(gates_2d, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "                    plt.colorbar(label='Gate Value')\n",
    "                    plt.xlabel('Flattened Input Channels × Kernel')\n",
    "                    plt.ylabel('Output Channel')\n",
    "                    plt.title(f'{layer_name} Weight Gates (Unstructured)')\n",
    "                plt.show()\n",
    "                break\n",
    "\n",
    "print(\"Structured Sparsity Pattern:\")\n",
    "visualize_conv_sparsity(structured_cnn, 'conv2')\n",
    "\n",
    "print(\"\\nUnstructured Sparsity Pattern:\")\n",
    "visualize_conv_sparsity(unstructured_cnn, 'conv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temperature Annealing Effects\n",
    "\n",
    "Demonstrating the importance of temperature scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fixed vs annealed temperature\n",
    "def train_with_temperature_strategy(strategy='fixed', n_epochs=50):\n",
    "    model = SparseMLP(input_dim=100, hidden_dim=50, output_dim=10, \n",
    "                      init_sparsity=0.0, temperature=0.5).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    if strategy == 'annealed':\n",
    "        temp_scheduler = TemperatureScheduler(1.0, 0.05, n_epochs//2)\n",
    "    \n",
    "    X = torch.randn(200, 100).to(device)\n",
    "    y = torch.randint(0, 10, (200,)).to(device)\n",
    "    \n",
    "    temps = []\n",
    "    sparsities = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if strategy == 'annealed':\n",
    "            temp = temp_scheduler.get_temperature(epoch)\n",
    "            update_temperatures(model, temp)\n",
    "        else:\n",
    "            temp = 0.5  # Fixed\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = F.cross_entropy(output, y) + 1e-3 * model.get_l0_loss()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        temps.append(temp)\n",
    "        stats = model.get_sparsity_stats()\n",
    "        avg_sparsity = np.mean([s['sparsity'] for s in stats.values()])\n",
    "        sparsities.append(avg_sparsity)\n",
    "    \n",
    "    return temps, sparsities\n",
    "\n",
    "# Run experiments\n",
    "fixed_temps, fixed_sparsities = train_with_temperature_strategy('fixed')\n",
    "annealed_temps, annealed_sparsities = train_with_temperature_strategy('annealed')\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(fixed_temps, label='Fixed', linewidth=2)\n",
    "ax1.plot(annealed_temps, label='Annealed', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Temperature')\n",
    "ax1.set_title('Temperature Schedule')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(fixed_sparsities, label='Fixed Temp', linewidth=2)\n",
    "ax2.plot(annealed_sparsities, label='Annealed Temp', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Sparsity')\n",
    "ax2.set_title('Learned Sparsity')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final sparsity - Fixed: {fixed_sparsities[-1]:.2%}, Annealed: {annealed_sparsities[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Basic L0 regularization** learns to identify informative features\n",
    "2. **LeNet-5 with L0** achieves high sparsity while maintaining accuracy\n",
    "3. **Feature selection** with L0 gates for high-dimensional data\n",
    "4. **L0 vs L1 comparison** shows L0 produces exact zeros while L1 only shrinks weights\n",
    "5. **Structured sparsity** prunes entire channels/filters for hardware efficiency  \n",
    "6. **Temperature annealing** improves convergence and final sparsity\n",
    "\n",
    "Key takeaways:\n",
    "- L0 regularization provides differentiable, learnable sparsity\n",
    "- Temperature scheduling is important for good results\n",
    "- Structured sparsity is more hardware-friendly than unstructured\n",
    "- L0 produces true sparsity (exact zeros) unlike L1/L2\n",
    "\n",
    "For production use, train on real datasets (MNIST, CIFAR-10, etc.) for meaningful results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}